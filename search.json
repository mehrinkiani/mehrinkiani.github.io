[
  {
    "objectID": "posts/llm_evaluations/blog.html",
    "href": "posts/llm_evaluations/blog.html",
    "title": "LLM Evaluations using Inspect",
    "section": "",
    "text": "Earlier this year, I took the course Mastering large language model (LLM) For Developers & Data Scientists (Maven 2024). The course primarily focussed on fine-tuning LLMs including best practices for fine-tuning a LLM, the maths behind it, and popular tools for fine-tuning LLMs. Apart from fine-tuning, the course also covered relevant content such as Retrieval Augment Generation (RAG), LLM evaluations and LLM deployment. In this post, I will share my notes from the course on LLM evaluations and will cover:\n\nA primer on LLM evaluations\nA primer on the tool Inspect (AI Safety Institute 2024)\nAn end to end example using Inspect to evaluate an LLMs’ responses for an AI driven medical diagnosis\n\nMuch of the content in this blog is my understanding of what the course covered though the example in Section 5 is not specific to the course. If you are already familiar with LLM evaluations, and the Inspect tool, please feel free to go to the example in Section 5 where I cover a step by step walk through for evaluating an LLMs’ responses for an AI driven medical diagnosis.\nIn case you would like more information on LLM evaluations beyond this post, I would highly recommend reading the cited references, and also taking the course (Maven 2024). Most of the content from the course is still available to watch freely on YouTube."
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#unit-evaluations",
    "href": "posts/llm_evaluations/blog.html#unit-evaluations",
    "title": "LLM Evaluations using Inspect",
    "section": "Unit Evaluations",
    "text": "Unit Evaluations\nIn some ways, we can think of unit tests for LLM evaluations as writing tests in a more traditional software development. Though this is more nuanced for LLM applications than a more traditional software because of the non-deterministic nature of LLM responses. Hence, even if some of unit evaluations are failing, it is acceptable unlike more typical software development.\nSome examples of unit tests that can be used to ensure LLM responses are as expected include:\n\nIs LLM response non-empty?\nIs LLM response numeric as expected, or string as expected?\nIs LLM response length within an expected range?\nIf multiple choice, is the response one of the expected choices?\n\nI like to think of unit evalualtions as more generic evals as compared to LLM as a judge evalualtions which are more domain specific evals. All four examples of unit tests do not require any knowledge of the task domain. In the example covered in Section 5, I will cover unit tests."
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#llm-as-a-judge",
    "href": "posts/llm_evaluations/blog.html#llm-as-a-judge",
    "title": "LLM Evaluations using Inspect",
    "section": "LLM as a Judge",
    "text": "LLM as a Judge\nLLM as a judge is when we use a bigger, more powerful LLM to test the quality of an LLM response being used in an application. A follow-up question could be why don’t we use the bigger, performant model in production to get quality responses: It is usually more expensive to fine-tune a bigger model, and also tends to be more expensive to deploy a bigger model in production.\nIn the example covered in Section 5, I will also cover LLM as a judge evaluation. The model I will use as a judge is OpenAI gpt-4. The LLM as a jugde evaluation can also be combined with human evaluations, explained next, to ensure that LLM judgment is in-line with domain experts."
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#human-evaluation",
    "href": "posts/llm_evaluations/blog.html#human-evaluation",
    "title": "LLM Evaluations using Inspect",
    "section": "Human Evaluation",
    "text": "Human Evaluation\nThis is where we can ask a domain expert to label LLM responses as good or bad to given user queries. We can then use these labelled data samples with domain expert preferences’ to improve model performance. Since human evaluations would require consultations from experts in a given domain, I will not be covering it in this post, though please read Hamel Hussain’s blog which provides examples for expert preferences and critique for a natural language query generator.\nNext, I will cover how we can use Inspect to run evaluations on an LLM."
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#dataset",
    "href": "posts/llm_evaluations/blog.html#dataset",
    "title": "LLM Evaluations using Inspect",
    "section": "Dataset",
    "text": "Dataset\nDataset is the data we want to apply our evaluations on. It should have at least two columns: user input that was sent to an LLM, and the response from the LLM. Inspect offers a first class support for loading csv files, and also natively supports laoding datasets from Hugging Face."
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#solver",
    "href": "posts/llm_evaluations/blog.html#solver",
    "title": "LLM Evaluations using Inspect",
    "section": "Solver",
    "text": "Solver\nSolvers are functions that transform dataset inputs such as prompt generation, call an LLM for generation such as when using another LLM as a judge, and act further on LLM output such as self-critique. Solvers can also be composed together as layers as shown in Figure 3, or can be a single layer with higher internal complexity. Some examples of solvers include:\n\nProviding system prompts\nPrompt engineering (e.g. chain of thought)\nModel generation\nSelf critique"
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#scorer",
    "href": "posts/llm_evaluations/blog.html#scorer",
    "title": "LLM Evaluations using Inspect",
    "section": "Scorer",
    "text": "Scorer\nScorers evaluates the output from solvers. Solvers may use text comparisons, model grading, or other custom schemes. In summary, scorers evaluate whether solvers were successful in finding the right output for the target defined in the dataset, and in what measure. Scorer then uses metrics to give a quantifiable score for the result of evaluation task such as accuracy 0.8 means that 80% of the responses from LLM met the evaluation criterion. Some examples of Scorers available in Inspect include:\n\nExtracting a specific answer out of a model’s completion output using heuristics.\nApplying a text similarity algorithm to see if a model’s response is close to target."
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#logs",
    "href": "posts/llm_evaluations/blog.html#logs",
    "title": "LLM Evaluations using Inspect",
    "section": "Logs",
    "text": "Logs\nLog files contain results of an evaluation output. Inspect has a VSCode extension that can be used to view log files seamlessly. Figure 4 shows a part of a log file generated by a validate task on a theory of mind dataset as outlined in Figure 3. The log file in Figure 4 shows\n\nFor each data instance a score as Correct (C) or Incorrect (I) is given by the scorer\nThe overall accuracy on right top indicates the ratio of number of data samples that resulted in correct evaluations\n\n\n\n\n\n\n\nFigure 4: An illustration for a log file generated for a “validate” task viewed using Inspect extension in VSCode."
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#data",
    "href": "posts/llm_evaluations/blog.html#data",
    "title": "LLM Evaluations using Inspect",
    "section": "Data",
    "text": "Data\nThe dataset used for evaluating consists of 1200 instances where each sample has text that explains the symptoms and label that tells corresponding disease name. The dataset is made available by Mistral AI, and can be found on Github Mistral-AI (2024). Table 1 has two entries from the dataset. The dataset contains symptoms for 24 diseases. A full list of disease names is included in SYSTEM_PROMPT in Figure 5.\n\n\n\nTable 1: Two data instances from symptom to disease dataset Mistral-AI (2024). The text column has the symptoms and label has corresponding disease name.\n\n\n\n\n\n\n\n\n\nlabel\ntext\n\n\n\n\nPsoriasis\nI have been experiencing a skin rash on my arms, legs, and torso for the past few weeks. It is red, itchy, and covered in dry, scaly patches.\n\n\nVaricose Veins\nLong durations of standing or walking have caused severe discomfort in my legs. It’s a burning ache that gets worse the longer I’m on my feet.\n\n\n\n\n\n\nA side note that when using an LLM as is, mistralai/Mistral-7B-Instruct-v0.3 in this case, with few-shot prompting we do not need labels for data instances as such i.e. name of disease in this case. Though when fine-tuning we would need such a labelled dataset."
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#model-responses",
    "href": "posts/llm_evaluations/blog.html#model-responses",
    "title": "LLM Evaluations using Inspect",
    "section": "Model Responses",
    "text": "Model Responses\nIn this section, I will use mistralai/Mistral-7B-Instruct-v0.3 to get diagnosis for each entry/symptom description in the dataset Table 1.\nI am using Mistral-7B-Instruct-v0.3 model because it is already fine-tuned to follow instructions. I also experimented a bit with Llama 3 7B though since it is not fine-tuned to follow instructions, the Llama responses comprised more of text generation than diagnosing a disease.\nFor this example, I included particular instructions I wanted Mistral-7B-Instruct-v0.3 to follow for medical disgnostics in SYSTEM_PROMPT. I also included an example response. In short, the instructions for LLM are to respond with a disease name that is most likely causing the symptoms as explained by a user in a prompt. The complete SYSTEM_PROMPT is shown in Figure 5.\n\n\n\n\nSYSTEM_PROMPT = \"\"\"\nYou are an expert medical professional named Medical AI. \n\nUsing the text provided by a user which is explaining symptoms of a disease, give your diagnosis. \n\nYou have to choose from one of the diseases below:\n\nPsoriasis                          \nVaricose Veins                     \npeptic ulcer disease               \ndrug reaction                      \ngastroesophageal reflux disease    \nallergy                            \nurinary tract infection            \nMalaria                            \nJaundice                           \nCervical spondylosis               \nMigraine                           \nHypertension                       \nBronchial Asthma                   \nAcne                               \nArthritis                          \nDimorphic Hemorrhoids              \nPneumonia                          \nCommon Cold                        \nFungal infection                   \nDengue                             \nImpetigo                           \nChicken pox                        \nTyphoid                            \ndiabetes  \n\nDo not add any other text except the disease name. \n\nFor example: \n\nUser input: The skin around my mouth, nose, and eyes is red and inflamed. It is often itchy and uncomfortable. There is a noticeable inflammation in my nails\nMedical AI: Psoriasis\n\nUser input:{{prompt}}\nMedical AI:\n\"\"\"\n\n\n\nFigure 5: SYSTEM_PROMPT as used in the example to query Mistral-7B-Instruct-v0.3 for getting diagnosis based on symptoms as entered by a user. The {prompt} will get replaced by symptoms from text field of dataset Mistral-AI (2024).\n\n\n\nTo use Mistral-7B-Instruct-v0.3 to get the responses, I installed mistral_inference, and downloaded the model from Hugging Face. Complete code block for getting Mistral-7B-Instruct-v0.3 response for each data instace in symptom to disease dataset is shown in Figure 6.\n\n\n\n\nimport pandas as pd\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\nmistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path, device = \"cuda\")\n\n# Data\n# Symptom2Disease.csd Dataset: !wget https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/data/Symptom2Disease.csv\ndataframe = pd.read_csv(\"Symptom2Disease.csv\")\nshuffled_dataframe = dataframe.sample(frac=1)\n\n\n# Iterate through prompts with progress bar\nfor index, row in tqdm(data.iterrows(), total=len(data)):\n    try:\n        # Get LLM response - modify this part according to your LLM API\n        prompt = SYSTEM_PROMPT.replace(\"{{prompt}}\", row[\"text\"])\n        completion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n        tokens = tokenizer.encode_chat_completion(completion_request).tokens\n\n        out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n        response_text = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n        \n                \n        # Store original prompt and response\n        results.append({\n            'prompt': row['text'],\n            'response': response_text\n        })\n\n        print(response_text)\n        \n        # Optional: Add delay to respect rate limits\n        time.sleep(0.1)\n        \n    except Exception as e:\n        print(f\"Error processing prompt at index {index}: {str(e)}\")\n        # Store error in results\n        results.append({\n            'prompt': row['prompt'],\n            'response': f\"ERROR: {str(e)}\"\n        })\n\n# Create new dataframe with results\nresults_df = pd.DataFrame(results)\n\n# Save to CSV\nresults_df.to_csv('llm_responses.csv', index=False)\n\n\n\nFigure 6: Code used for getting mistralai/Mistral-7B-Instruct-v0.3 responses for system to disease dataset Mistral-AI (2024)."
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#unit-evaluations-1",
    "href": "posts/llm_evaluations/blog.html#unit-evaluations-1",
    "title": "LLM Evaluations using Inspect",
    "section": "Unit Evaluations",
    "text": "Unit Evaluations\nThe first form of evaluations to consider are unit evaluations that ensure that our LLM is responding with expected response. For this example we are using mistralai/Mistral-7B-Instruct-v0.3 to diagnose a disease based on given symptoms. In order to evaluate Mistral-7B-Instruct-v0.3 responses for medical diagnosis using unit evaluation, we can test for following unit evaluations:\n\nResponse is non null\nResponse only contains strings\nResponse has one of the disease names as outlined in SYSTEM_PROMPT in Figure 5.\n\nThough the first two are not domain specific evaluations, the third one can be deemed somewhat domain specific though still no domain knowledge is required for third evaluation per se. Hence for unit evaluations, we do not require to use another LLM as a judge just yet. An excerpt from code for unit evaluation is shown in Figure 7.\n\n\n\n\nclass TestSample:\n    def __init__(self, data: Dict):\n        self.data = data\n        self.test_results = {}\n    \n    def run_tests(self):\n        # Test 1: Non-Empty Response\n        self.test_results['non_empty_response'] = not pd.isna(self.data['response']) and self.data['response'] != ''\n        \n        # Test 2: String Type\n        self.test_results['all_string'] = isinstance(self.data['response'], str)\n        \n        # Test 3: Valid Disease Name\n        try:\n            if self.data['response'] in POSSIBLE_DISEASES or POSSIBLE_DISEASES in self.data['response']:                \n                self.test_results['valid_disease_name'] = True        \n        except:\n            self.test_results['valid_disease_name'] = False\n            \n        return self.test_results\n\n\n\nFigure 7: An excerpt that shows the unit evaluations being used to assert that response from LLM for diagnosis is non-empty, contains all strings, and contains one of the possible disease names.\n\n\n\nI ran the evaluation on all 1200 data samples, i.e. responses obtained from Mistral-7B-Instruct-v0.3, and saved results in a .csv file for later use purposes. I also generated a bar plot shown in Figure 8 where we can see that all responses from Mistral-7B-Instruct-v0.3 were non-empty, and contained strings only. Though around 600 samples failed to include a disease name in response.\n\n\n\n\n\n\nFigure 8: Unit evaluations run on Mistral-7B-Instruct-v0.3 model response passed non-empty response, and all string response, though almost half of responses failed to include a disease name.\n\n\n\nIn Figure 9, I have listed some responses that are failing the “disease name in response” test. A quick look tells that these responses did indeed have a disease name though failed the unit evaluations because probably the case didn’t match between model response and the name of the disease in POSSIBLE_DISEASES.\n\n\n\nResponse does not contain valid disease name:\n,response\n2,Gastroesophageal Reflux Disease (GERD)\n5,Peptic Ulcer Disease\n11,\"Psoriasis (for the first user input)\nVaricose Veins (for the second user input)\n(For the third user input, the symptoms provided are not specific to any of the listed diseases. It could be a sign of various conditions such as eczema, seborrheic\"\n12,Psoriatic Arthritis (This condition is a combination of psoriasis and arthritis.)\n\n\nFigure 9: A few of the responses that failed the “response contains a disease name” test. As we can see that in each reponse there does seem to be a disease name though our unit test is case sensitive which is generating a lot of false positives.\n\n\n\nFor the next iteration of unit evaluations, I updated the “response contains a disease name” test to be case insensitive and the new results are as shown in Figure 10. There is a ~33% improvement in “response contains a disease name” test results. Some of the responses that still failed the test are listed in Figure 11.\n\n\n\n\n\n\nFigure 10: Unit evaluations run on Mistral-7B-Instruct-v0.3 model response passed non-empty response, and all string response, though almost half of responses failed to include a disease name.\n\n\n\n\n\n\nResponse does not contain valid disease name:\n,response\n2,Gastroesophageal Reflux Disease (GERD)\n11,\"Psoriasis (for the first user input)\nVaricose Veins (for the second user input)\n(For the third user input, the symptoms provided are not specific to any of the listed diseases. It could be a sign of various conditions such as eczema, seborrheic\"\n12,Psoriatic Arthritis (This condition is a combination of psoriasis and arthritis.)\n13,Jaundice (possibly associated with Hepatitis or other liver diseases)\n18,Jaundice (possibly due to Hepatitis or Gallstones)\n20,Common Cold or Bronchial Asthma (depending on the persistence and severity of symptoms)\n28,Flu (Influenza)\n43,Common Cold or Flu (Influenza)\n46,Flu (Influenza)\n\n\nFigure 11: A few of the responses that failed the case-insensitive “response contains a disease name” test. As we can see that in each reponse there does seem to be a disease name though some samples do not seem to have same disease name as in POSSIBLE_DISEASE list such as Flu.\n\n\n\nIn a furture iteration of the “response contains a disease name” test, we can perhaps add synonyms for common disease such as Flu is same as Cold, or Common Cold. We can also consider to revise SYSTEM_PROMPT to include multiple names for a given disease. Also, it is worthwhile to mention that despite stating in SYSTEM_PROMPT to only include a disease name as response there is still some additional text as seen in Figure 11. We can try aligning it further by adding more examples of expected response in SYSTEM_PROMPT though at some point a change in SYSTEM_PROMPT alone might not be sufficient, and this is when we would like to consider RAG or fine-tuning.\nNext, I will cover LLM as a Judge for evaluating Mistral-7B-Instruct-v0.3 responses."
  },
  {
    "objectID": "posts/llm_evaluations/blog.html#llm-as-a-judge-1",
    "href": "posts/llm_evaluations/blog.html#llm-as-a-judge-1",
    "title": "LLM Evaluations using Inspect",
    "section": "LLM as a Judge",
    "text": "LLM as a Judge\nFor using an LLM as a judge, we can query another, more powerful LLM and “make it a judge” to assess the response quality. Inspect support most of the popular models such as OpenAI, Anthropic, Google, Mistral. For a complete list of supported models please read here. Please note, we will need an API key for the model we want to use as our judge.\nThe critique prompt should include background and instructions for the judge LLM to use as a guide for its critique of Mistral-7B-Instruct-v0.3 responses. Figure 12 is an excerpt from critique prompt that I used in this example though please note the critique prompt is generated using an LLM and may not be representative of best practices in medical diagnostics.\n\n\n\n## Background\n\nA medical diagnosis system should evaluate symptoms and provide accurate disease predictions. The system should consider:\n\n1. Primary symptoms (major indicators of the disease)\n2. Secondary symptoms (supporting symptoms)\n3. Location and distribution of symptoms\n4. Symptom severity, duration, and progression\n5. Common comorbidities and complications\n6. Risk factors and patient demographics\n7. Seasonal patterns (when applicable)\n8. Symptom triggers and alleviating factors\n\n## Instructions\n\nYou are an EXPERT medical professioanl evaluating symptom-to-disease predictions. You understand symptom patterns, disease progression, and diagnostic criteria for accurate disease identification. You understand the nuances of symptom-to-disease predictions, including what is likely to be most important symptoms for a given disease.\n\nYou are given the following two inputs: (1) SYMPTOMS, (2) PREDICTION. Your job is to evaluate and critique the PREDICTION relative to the provided SYMPTOMS. \n\nThe critiques must be provided in the same json format as provided in the examples below:\n\nInputs:\n1. Patient symptom description (SYMPTOMS)\n2. Predicted disease (PREDICTION)\n\nThe critique system should evaluate:\n- Symptom pattern matching\n- Presence of disease-specific indicators\n- Appropriate symptom timing and progression\n- Correct interpretation of constitutional symptoms\n- Recognition of complicating factors\n- Proper differentiation from similar conditions\n- Consideration of risk factors and demographics\n- Identification of red flags requiring immediate attention\n\nFor the below SYMPTOMS and PREDICTION provide a critique as JSON in the format {\"critique\": \"...\", \"outcome\": \"good\"|\"bad\"} as shown above. Only include the critique in your response (do not include any additional text before or after the critique).\n\nOutcomes should be marked \"good\" when predictions accurately match symptom patterns and \"bad\" when key diagnostic features are missed or misinterpreted.\n\n\n---\n\nSYMPTOMS: \"I've been having severe stomach pain, especially when hungry. The pain improves after eating but comes back after a few hours. I sometimes feel nauseous and have noticed dark stools.\"\n\nPREDICTION: Peptic Ulcer Disease\n\n{\"critique\": \"Excellent prediction. Key supporting factors: 1) Cyclical pain pattern worsening with hunger and improving with food is classic for peptic ulcer, 2) Epigastric pain location is typical, 3) Associated nausea is common, 4) Dark stools suggest possible gastrointestinal bleeding, a known complication. The symptom complex strongly indicates peptic ulcer disease rather than simple GERD or gastritis.\", \"outcome\": \"good\"}\n\n---\n\n\nFigure 12: Some exceprt from critique prompt that I used for LLM as a judge. Disclaimer: I have used an LLM to generate example critiques which may not be representative of best practises in medical diagnostics. This illustrative critique is to just show case how one LLM can be used as a judge to evaluate another LLMs responses.\n\n\n\nI used OpenAI gpt4 model as a judge to critique 50 Mistral-7B-Instruct-v0.3 responses to keep a tab on the cost involved. The critique evaluation resulted in an accuracy of 0.38 i.e. for ~40% samples LLM as a judge, gpt4, responses are in agreement with base LLM, Mistral-7B-Instruct-v0.3, responses. We can gain insight from the LLM as a judges critiques to learn where our base model,Mistral-7B-Instruct-v0.3, is struggling for example in Figure 13 judge LLM thinks symptoms are more representative of Hypertension than Migraine, and also highlights that more context would help to conclude confidently between Migraine and Hypertension.\nBy reading more evaluations of where the LLM as a judge agrees and differs with Mistral-7B-Instruct-v0.3 it would help to gain insight on the strength and weaknesses for using Mistral-7B-Instruct-v0.3as a base LLM for medical diagnosis. By leveraging the feedback from LLM as a judge, the decision to whether base LLM responses contain sufficient context and depth for performing task at hand or whther we need to give more supporting documents via RAG or perhaps fine-tune can be made in a timely fashion. Though it is important to highlight that the judge is also another LLM and may hallucinate, or provide biased critique.\n\n\n\n\n\n\nFigure 13: A sample showing critique for a medical diagnosis that judge LLM thought was a poor diagnosis. It includes explanation for its critique.\n\n\n\nThe critique from an LLM can also be used in tandem with subject experts, medical professionals in this case, to curate data for fine-tuning. Though again this should be used with caution as it can help propagate human or LLM bias in fine-tuning dataset which in turn would result in skewed responses from a fine-tuned model."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hello! and welcome to my Machine Learning (ML) blog!\nI plan using my blog to share what I learn."
  },
  {
    "objectID": "about.html#work-experience",
    "href": "about.html#work-experience",
    "title": "About",
    "section": "Work Experience",
    "text": "Work Experience\nI work as a Machine Learning Scientist. Part of my reserch work focussed on ModelScan: It is an OSS project that scans Machine Learning (ML) models to determine if they contain unsafe code, i.e. scans for Model Serialization Attack (MSA).\n\nBuild a proof of concept for model serialization attacks for the aforementioned ML libraries\nWrote Jupyter Notebooks to demonstrate MSA on Tensorflow, Keras, PyTorch, XGBoost models.\nDevelop ModelScan to detect model serialization attacks without loading the model for the aforementioned ML frameworks"
  },
  {
    "objectID": "about.html#educational-background",
    "href": "about.html#educational-background",
    "title": "About",
    "section": "Educational Background",
    "text": "Educational Background\nI have a PhD in Computer Science from University of Essex, England. My research work focussed on developing novel ML algorithms to inform our understanding of developmental cognitive neuroscience. More details of my research can be found on my Google Scholar."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Machine Learning Blog",
    "section": "",
    "text": "LLM Evaluations using Inspect\n\n\n\n\n\n\n\n\n\n\n\nDec 20, 2024\n\n\nMehrin Kiani\n\n\n\n\n\n\nNo matching items"
  }
]