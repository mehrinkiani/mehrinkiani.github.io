{
  "hash": "1cfd9c1c916e128a9adeac7772af6756",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: LLM Evaluations using Inspect\nauthor: Mehrin Kiani\ndate: '2024-12-20'\nbibliography: references.bib\nformat:\n  pdf:\n    pdf-engine: pdflatex\n    code-fold: false\n    toc: true\nexecute:\n  echo: true\n---\n\n# Introduction \n\nEarlier this year, I took the course Mastering large language model (LLM) For Developers & Data Scientists [@mastering_llms_2024]. The course is packed with a lot of content about fine-tuning, RAG, LLM evaluations and more. In this post, I will share my notes from the course on LLM evaluations and cover:\n\n* A primer on LLM evaluations \n* A primer on the tool Inspect [@inspect-2024] used for LLM evaluations\n* An end to end example using Inspect to evaluate LLM responses for a symptom to disease classification task\n    \n\n# LLM Evaluations \nIn some ways, we can think of LLM evaluations as writing tests in a more traditional software development. The motivation for evaluating a LLM response is to ensure that response from a LLM is as expected, and that if we are fine-tuning our LLM the responses have improved post fine-tuning.\n\nFor example, before fine-tuning we are classifying 60% samples correctly. Afetr fine-tuning we should expect that this number would hopefully improve. But how do we quantify this using evaluaitons? This is where LLM evaluations come in. \n\nThink of splitting your dataset on train, validation, and test dataset like you would for a more traditional machine learning project. Fine-tune your LLM on train dataset, and then run evaluations using validation dataset.\n\nAs your dataset evolves, or your LLM changes, you would need to be able to iterate on your evaluations. Also, it would help to keep in mind that writing evaluations is an iteratative process.\n\nThis section is a rather brief summary of evaluation posts from [Hamel Hussains' blog](https://hamel.dev/blog/posts/evals/). I would recommend reading Hamels blog if you are not initiated on LLM evaluations yet, and would also like some inspiration for writing evaluations using a case study. Other resources on learning more about  [Eugene Yan blog](https://eugeneyan.com/writing/evals/), and Shreya Shankar blog. Eugene blog covers different evaluations for different tasks  . Finally [Shreyas blog](https://www.sh-reya.com/blog/ai-engineering-flywheel/) provides a data flywheel concept that gives a more wholistic view to think and implement data flywheels for LLM applications. I plan covering them in a later post, hopefully with some use case. \n\nThere are two main things that I would like to highlight:\n\n* Understand your dataset and be able to look at your dataset <- this is where Inspect will help\n* Once we have a greater understanding of our data, we can write better evals. \n\nBroadly speaking, there are three levels in which an LLM evaluation can be categories.  \n\n* Unit tests \n* LLM as a judge\n* Human Evaluation\n\nNext, I will cover how we can use Inspect to run evaluations on an LLM.\n\n# Inspect  \n\nOther options for evals include OpenAI evals Eluether LM Eval Harness\nThe code referenced in the post can be found here: [Inspect LLM Workshop](https://github.com/jjallaire/inspect-llm-workshop)\n\nInspect support most of the popular models such as OpenAI, ANthropic, Google, Mistral. Fora full list please read [here](https://inspect.ai-safety-institute.org.uk/models.html)\n\nInspect [@inspect-2024] is an open source framework that can be used for evaluating LLM responses. It is created by UK AI Safety Institute. At a high level inspect has a `Task` object that combines three main components of Inspect evaluations: Datasets, Solvers and Scorers as illustrated in Fig @fig-inspect-flowchart. \n\n`Task` generats a `Log` that can viewed \n\n\n::: {#fig-inspect-flowchart}\n\n\n```{mermaid}\n%%| echo: false\nflowchart TB\n    subgraph Task\n        subgraph Component1[Datasets]\n            A[Labelled input and target columns]\n        end\n        subgraph Component2[Solvers]\n            B[Can have multiple levels]\n        end\n        subgraph Component3[Scorers]\n            C[Evaluates the response from Solvers]\n        end        \n        Component1 --> Component2\n        Component2 --> Component3\n    end\n    subgraph Component4[Logs]\n            D[Logs]\n    end\n    Task --> Component4\n```\n\n\nAn illustration for Inspect object `Task` that combines the three basic components of Inspect: Datasets, Solvers, and Scorers. \n:::\n\nALl three components are encapsulated in a `Task`. \nThe first step is to load the data. Dataset is the data you want to apply your evaluations on. Inspect offere a first class support for loading csv files, and also natively supports laoding datasets from Hugging Face. For more options on dataset, please read more here:\n\nOnce we have the dataset loaded then we need to determine what sort of evaluations do we want to run on it. The type of evalution we want to run will determie the scorer we want to use. \n\nThis is closely tied to the task an LLM application is performing. So for example, if an LLM application is used for sentiment classification, then we want to evaluate whether the sentiment classification. If LLM application is doing a multiple choice question answer then we want to different scorer. \n\nSolver: Functions that transform dataset inputs, call the model for\ngeneration, and act further on model output. Can be\ncomposed together as layers, or can be a single layer with\nhigher internal complexity. A solver is a Python function that takes a TaskState and generate function, and then transforms and returns the TaskState (the generate function may or may not be called depending on the solver).\n\nProviding system prompts\nPrompt engineering (e.g. chain of thought)\nModel generation\nSelf critique\nMulti-turn dialog\nRunning an agent scaffold\n\nScorer: Evaluates final output of solvers. May use text comparisons,\nmodel grading, or other custom schemes. Scorers evaluate whether solvers were successful in finding the right output for the target defined in the dataset, and in what measure. Scorers generally take one of the following forms:\n\nExtracting a specific answer out of a model’s completion output using a variety of heuristics.\n\nApplying a text similarity algorithm to see if the model’s completion is close to what is set out in the target.\n\nUsing another model to assess whether the model’s completion satisfies a description of the ideal answer in target.\n\nUsing another rubric entirely (e.g. did the model produce a valid version of a file format, etc.)\n\n::: {.cell execution_count=1}\n``` {.python .cell-code}\nfrom inspect_ai import Task, task\nfrom inspect_ai.dataset import example_dataset\nfrom inspect_ai.scorer import model_graded_fact\nfrom inspect_ai.solver import (               \n  prompt_template, generate, self_critique   \n)                                             \n\nDEFAULT_PROMPT=\"{prompt}\"\n\n@task\ndef theory_of_mind():\n    return Task(\n        dataset=example_dataset(\"theory_of_mind\"),\n        solver=[\n          prompt_template(DEFAULT_PROMPT),\n          generate(),\n          self_critique()\n        ],\n        scorer=model_graded_fact()\n    )\n```\n:::\n\n\n## Logs:\n\nView the results of the evaluation output. You can install the VSCode extension for inspect to view the logs. That \n\nCapture all context required to debug, analyse, and reproduce evaluations\nPython API for computing on log file contents\nLog viewer for interactive exploration of eval results\n\n# Example: Evaluating disease detection by LLM\n\nIn this section, I will cover how I developed LLM evaluations for a hypothetical LLM application that is being used for detecting a disease given some symptoms. \n\nThe model I am using is: Mistral \n\nThe example is run locally on VSCode. A complete code is uploaded here: \n\n## Data     \n\n\nThe dataset used for evaluating consists of 1200 instances where each sample has `text` that explains the symptoms and `label` that tells corresponding disease name. There are a total of 24 unique diseases. @tbl-sample-dataset has two entries from the dataset. \n\n@mistral2024,\n\n::: {.cell execution_count=2}\n\n::: {#fig-dataset .cell-output .cell-output-display execution_count=14}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>label</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>280</th>\n      <td>280</td>\n      <td>Dengue</td>\n      <td>I have developed a skin rash that covers my en...</td>\n    </tr>\n    <tr>\n      <th>584</th>\n      <td>284</td>\n      <td>Acne</td>\n      <td>I awoke this morning to see a horrible rash on...</td>\n    </tr>\n    <tr>\n      <th>397</th>\n      <td>97</td>\n      <td>Common Cold</td>\n      <td>I've had a terrible cough and cold for days. M...</td>\n    </tr>\n    <tr>\n      <th>743</th>\n      <td>143</td>\n      <td>Migraine</td>\n      <td>Along with excessive appetite, a stiff neck, d...</td>\n    </tr>\n    <tr>\n      <th>982</th>\n      <td>82</td>\n      <td>allergy</td>\n      <td>I have breathing problems and become out of br...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n\nA sample from the input dataset\n:::\n:::\n\n\n| label | text |\n|---------|:-----|\n| Psoriasis     | I have been experiencing a skin rash on my arms, legs, and torso for the past few weeks. It is red, itchy, and covered in dry, scaly patches.  |   \n| Varicose Veins    | sLong durations of standing or walking have caused severe discomfort in my legs. It's a burning ache that gets worse the longer I'm on my feet.|   \n  \n\n: A sample from the input dataset {#tbl-sample-dataset}{tbl-colwidths=\"[25,75]\"}\n\n## Unit Evals \nWe want to ensure that our LLM is responding with expected response. We can think of unit evals as writing unit tests. For this example, we can think of generic and domain-specific evaluations as follows:\n\nGeneric evals:\n\n* Are columns used?\n* Response is non null \n* Response is string \n\nDomain specific evals:\n\n* Response has maximum how many characters \n* Response has one of the disease names \n* Lets also add a unknown category in case a user asks \n\nAlso we can ask LLM to also give us some ideas for generating evals. \n\n\n\n## LLM as a judge \n\nWe can query another LLM and ask is to evaluate. We would need to give it some criterion. \n\n\n\n\n## Human as a judge \n\nFor a demonstration of a line plot on a polar axis, see @fig-polar.\n\n::: {.cell execution_count=3}\n``` {.python .cell-code}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0, 2, 0.01)\ntheta = 2 * np.pi * r\nfig, ax = plt.subplots(\n  subplot_kw = {'projection': 'polar'} \n)\nax.plot(theta, r)\nax.set_rticks([0.5, 1, 1.5, 2])\nax.grid(True)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![A line plot on a polar axis](blog_files/figure-pdf/fig-polar-output-1.pdf){#fig-polar fig-pos='H'}\n:::\n:::\n\n\n# Conclusion\n\nPerhaps we can think of evaluations also in terms of user input. \n\n### References {-}\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "blog_files/figure-pdf"
    ],
    "filters": []
  }
}