{
  "hash": "75ebc1861d9c950c547b6e8be83abef7",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: LLM Evaluations using Inspect\nauthor: Mehrin Kiani\ndate: '2024-12-20'\nbibliography: references.bib\nformat:\n  html:\n    code-fold: false\n    toc: true\nexecute:\n  echo: true\n---\n\n# Introduction \n\nEarlier this year, I took the course Mastering large language model (LLM) For Developers & Data Scientists [@mastering_llms_2024]. The course primarily focussed on fine-tuning LLMs including best practices for fine-tuning a LLM, the maths behind fine-tuning a LLM, and popular tools for fine-tuning LLMs. Apart from fine-tuning, the course also covered relevant content such as Retrieval Augment Generation (RAG), LLM evaluations and LLM deployment. In this post, I will share my notes from the course on LLM evaluations and will cover:\n\n* A primer on LLM evaluations \n* A primer on the tool Inspect [@inspect-2024] \n* An end to end example using Inspect to evaluate an LLMs' responses for a symptom to disease daignosis  \n    \nMuch of the content in this blog is my understanding/notes of what the course covered though the example in @sec-example is not specific to the course. If you are already familiar with LLM evaluations, and the Inspect tool, please feel free to go to the example in @sec-example where I cover a step by step walk through for evaluating an LLMs' responses. \n\nIn case you would like more information on LLM evaluations beyond this blog, I would highly recommend reading the cited references, and also taking the course [@mastering_llms_2024]. Most of the content from the course is still available to watch freely on YouTube. \n\n# Why do we need LLM Evaluations?\n\nFor the uninitated, LLM evaluations are important to calibrate the performance of an LLM for a given task. For example, if we want to use an LLM for an AI driven medical diagnosis given some symptoms by a user there are multiple techniques available for adapting an LLM such as: 1) use LLM as is out of the box i.e. zero shot prompting, 2) few-shot learning, 3) RAG, or 4) fine-tune an LLM. Though how do we decide which technique(s) is giving us the best response from an LLM for a given task? \n\nApart from the choice between different techniques for adapting an LLM to a certain task, if we want to calibrate other \"hyperparameters\" for an LLM such as one system prompt versus another, or if we choose a different base LLM say Mistral 7B instead Llama 3 7B, how can we conclude which model or system prompt is giving us better results? \n\nThe aforementioned questions provide the motivation for LLM evaluations: a framework for calibrating the performance of an LLM for a given task and can also be leveraged to choose \"hyperparameters\" for optimal LLM performance as illustrated in @fig-why-llm-evals. This is also why we should version our system prompts and LLM evaluation results together. \n\n\n::: {#fig-why-llm-evals}\n\n\n```{mermaid}\n%%| echo: false\nflowchart LR\n  A(User enters symptoms) --> B(Large Language Model application <br> gives medical diagnosis)\n  F(System Prompt v1) -.-> B\n  G(System Prompt v2) -.-> B\n  B --> C1(Medical diagnosis using system prompt 1)\n  B --> C2(Medical diagnosis using system prompt 2)\n  C1 --> D{LLM evaluation help answer <br>  which response is better?} \n  C2 --> D{LLM evaluation help answer <br> which response is better?} \n  D --> E(System Prompt 1 is better)\n\n  style A color:purple\n  style B color:green\n  style C1 color:red\n  style F color:red\n  style C2 color:blue\n  style G color:blue\n  style E color:red\n```\n\n\n\nAn over-simplified illustration for the importance of Large Language Model (LLM) evaluations in quantifying the performance of an LLM. LLM evaluations can also be leveraged as a data fly-wheel for iterativetly improving an LLM performance. For example, in this case the choice between system prompt 1 or 2 can be informed by LLM evaluations. Likewise, choice between which base models such as Mistral 7B or Llama 3 7B to use can also be made using LLM evaluations.   \n:::\n\n\n# LLM Evaluations \n\nLLM evaluations are an integral component for quantifying the performance of an LLM. As our dataset evolves, or our use-cases for LLM based product changes, we would need to be able to iterate on our evaluations to improve LLM performance. \n\nAt the core of good, quality LLM evaluation is an understanding of data. As Hamel mentions [[blog]](https://hamel.dev/blog/posts/evals/), a sign of quality unit tests is when a LLM struggles to pass them for these failure modes become problems we can solve with techniques like fine-tuning later on. I would also like to emphasize that writing evaluations is an iterative process because of the cyclic dependency between understanding data and writing quality evaluations ♻️\n\n* Understand data by reading data samples, finding patterns, edge cases in data etc.\n* Once we have a greater understanding of our data, we can write better evals. \n* Better evaluations will help increase our understanding of data\n\nThis section is intentionally a rather brief summary of LLM evaluations. For a more in-depth overview, I would recommend reading [Hamel Hussain's blog](https://hamel.dev/blog/posts/evals/) that gives a comprehensive overview of LLM evaluations as well as inspiration for writing evaluations using a case study. I would also recommend [Eugene Yan's blog](https://eugeneyan.com/writing/evals/) covers different evaluations for different tasks and [Shreya Shankar's blog](https://www.sh-reya.com/blog/ai-engineering-flywheel/) provides a wholistic data flywheel concept to implement evaluations for LLM applications. \n\nNext, I will cover the three fundamental types of LLM evaluations. In example covered in @sec-example, I will use Inspect for LLM evaluations though there are other tools available for LLM evals such as [OpenAI evals](https://github.com/openai/evals), [Eluether LM Eval Harness](https://github.com/EleutherAI/lm-evaluation-harness).\n\n* Unit tests\n* LLM as a judge\n* Human evaluation\n\n## Unit Evaluations \nIn some ways, we can think of unit tests for LLM evaluations as writing tests in a more traditional software development. Though this is more nuanced for LLM applications than a more traditional software because of the non-deterministic nature of LLM responses. Hence, even if some of unit evaluations are failing, it is acceptable unlike more typical software development.\n\nSome examples of unit tests that can be used to ensure LLM responses are as expected include:\n\n - Is LLM response non-empty? \n - Is LLM response numeric as expected, or string as expected?\n - Is LLM response length within an expected range? \n - If multiple choice, is the response one of the expected choices?\n\nI like to think of unit evalualtions as more generic evals as compared to LLM as a judge evalualtions which are more domain specific evals. All four examples of unit tests do not require any knowledge of the task domain. In the example covered in @sec-example, I will cover unit tests.\n\n## LLM as a Judge\nLLM as a judge is when we use a bigger, more powerful LLM to test the quality of an LLM response being used in an application. A natural question is why don't we use the bigger, performant model in production to get quality responses: It is usually more expensive to fine-tune a bigger model, and also tends to be more expensive to deploy a bigger model in production. \n\nIn the example covered in @sec-example, I will also cover LLM as a judge evaluation. The model I will use as a judge is OpenAI `gpt-4`. The LLM as a jugde evaluation can also be combined with human evaluations to ensure that LLM judgment is in-line with domain experts. \n\n## Human Evaluation\n\nThis is where we can ask a domain expert to label LLM responses as good or bad to given user queries. We can then use these labelled data samples with domain expert preferences' to improve model performance. Since human evaluations would require consultations from experts in a given domain, I will not be covering it in this post, though please read [Hamel Hussain's blog](https://hamel.dev/blog/posts/evals/) which provides examples for expert preferences and critique for a natural language query generator.\n\nNext, I will cover how we can use Inspect to run evaluations on an LLM.\n\n# Inspect  \n\nInspect [@inspect-2024] is an open source framework that can be used for evaluating LLM responses. Inspect is created by UK AI Safety Institute. At a high level inspect has a `Task` object that combines three main components of Inspect evaluations as shown in @fig-inspect-flowchart: \n\n* Datasets\n* Solvers \n* Scorers \n\nOnce we have a dataset from our LLM that we want to evaluate, we need to determine the type of evaluations we want to run on it. A dataset from an LLM mean we have a user query/text given to LLM, and corresponding response from LLM recorded for each given prompt.\n\nThe type of evalution we want to run on a data will determine the solver and scorer we want to use, and this is closely tied to the task our LLM application is performing. So for example, if our LLM application is used for sentiment classification, then we want to run a classification evaluation. If an LLM application is choosing an option from multiple choices, then we can perhaps use `multiple_choice()` solver with `choices()` scorer. Inspect offers a number of options to choose from for both scorer and solvers. \n\nOnce a `Task` is run successfully, it generates a `Log` file that can be viewed natively in VSCode if Inspect extension is installed as shown in @fig-inspect-flowchart. If Inspect extension is not installed, log file can also be viewed in a browser using `$ inspect view`.\n\n::: {#fig-inspect-flowchart}\n\n\n```{mermaid}\n%%| echo: false\nflowchart TB\n    subgraph Task[Task]\n        subgraph Component1[Dataset]\n            A(Labelled input and response columns)\n        end\n        \n        subgraph Component2[Solvers]\n            B1(Prompt templating)\n            B2(Generate)\n            B3(Self critique)\n        end\n        \n        subgraph Component3[Scorers]\n            C(Evaluates the response from solvers)\n        end\n        \n        Component1 --> Component2\n        Component2 --> Component3\n    end\n    \n    subgraph Component4[Logs]\n        D(Task completion generates a log file)\n    end    \n    Task --> Component4    \n\n    style Task rx:10,ry:10,color:purple\n    style Component1 rx:10,ry:10,color:green\n    style Component2 rx:10,ry:10,color:red\n    style Component3 rx:10,ry:10,color:blue\n    style Component4 rx:10,ry:10,color:brown\n```\n\n\nAn illustration for Inspect object `Task` that combines the three basic components of Inspect: Datasets, Solvers, and Scorers. Upon successful completion of a task, a log file is generated that can be viewed in VSCode if Inspect extension is installed. @fig-log-file shows a sample log file.\n:::\n\nThe Inspect documentation covers standalone examples, such as one shown in @fig-standalone-theory-of-mind, for different datasets, and corresponding solvers and scorers. In the following sections, I provide a brief summary of the Task components, though would recommend reading  [Inspect documentation](https://inspect.ai-safety-institute.org.uk/) for more details.\n\n::: {#fig-standalone-theory-of-mind}\n\n::: {#e5977724 .cell execution_count=1}\n``` {.python .cell-code}\n# theory.py\nfrom inspect_ai import Task, task\nfrom inspect_ai.dataset import example_dataset\nfrom inspect_ai.scorer import model_graded_fact\nfrom inspect_ai.solver import (               \n  prompt_template, generate, self_critique   \n)                                             \n\nDEFAULT_PROMPT=\"{prompt}\"\n\n@task\ndef theory_of_mind():\n    return Task(\n        dataset=example_dataset(\"theory_of_mind\"),\n        solver=[\n          prompt_template(DEFAULT_PROMPT),\n          generate(),\n          self_critique()\n        ],\n        scorer=model_graded_fact()\n    )\n```\n:::\n\n\nA complete, stand alone example from Inspect documentation that outlines the usage of three basic components, encapuslated in `Task`: Datasets, Solvers, and Scorers for Theory of mind. The decorator `@task` helps find Inspect the task to run. In this case, there are three solvers applied to theory of mind dataset. The task can be run using `$ inspect eval theory.py --model openai/gpt-4`. Please note you will need `OPENAI_API_KEY` in your environment to run the task. \n\n:::\n\n## Dataset\nDataset is the data you want to apply your evaluations on. It should have at least two columns: user input that was sent to an LLM, and the response from the LLM. Inspect offers a first class support for loading csv files, and also natively supports laoding datasets from Hugging Face. \n\n## Solver \nSolvers are functions that transform dataset inputs such as prompt generation, call an LLM for generation, and act further on LLM output such as self-critique. Solvers can also be composed together as layers as shown in @fig-standalone-theory-of-mind, or can be a single layer with higher internal complexity. Some examples of solvers include:\n\n* Providing system prompts\n* Prompt engineering (e.g. chain of thought)\n* Model generation\n* Self critique\n\n## Scorer\nScorers evaluates the output from solvers. Solvers may use text comparisons, model grading, or other custom schemes. In summary, scorers evaluate whether solvers were successful in finding the right output for the target defined in the dataset, and in what measure. Some examples of Scorers available in Inspect include:\n\n* Extracting a specific answer out of a model’s completion output using heuristics.\n\n* Applying a text similarity algorithm to see if a model’s response is close to target.\n\n## Logs\n\nLog files contain results of an evaluation output. Inspect has a VSCode extension that can be used to view log files seamlessly. @fig-log-file shows a part of a log file generated by a validate task on a theory of mind dataset as outlined in @fig-standalone-theory-of-mind. The log file in @fig-log-file shows\n\n* For each data instance a score as Correct (C) or Incorrect (I) is given by the scorer\n* The overall accuracy on right top indicates the number of data samples that resulted in correct evaluations \n\n![An illustration for a log file generated for a \"validate\" task viewed using Inspect extension in VSCode.](images/inspect_log_example.png){#fig-log-file}\n\n# Example: Evaluating LLM Responses on Symptom to Disease Diagnosis {#sec-example}\n\nIn this section, I will cover how I developed LLM evaluations for a hypothetical LLM application that is being used for disease diagnostics given some symptoms. The base LLM I am using is [`mistralai/Mistral-7B-Instruct-v0.3`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3). \n\nThe example is run locally on VSCode, and I have installed Inspect extension to view log files. \n\n## Data     \nThe dataset used for evaluating consists of 1200 instances where each sample has `text` that explains the symptoms and `label` that tells corresponding disease name. The dataset is made available by Mistral AI, and can be found on Github @mistral2024. @tbl-sample-dataset has two entries from the dataset. The dataset contains symptoms for 24 diseases. A full list of disease names is included in `SYSTEM_PROMPT` in @fig-system-prompt.\n\n| label | text |\n|---------|:-----|\n| Psoriasis     | I have been experiencing a skin rash on my arms, legs, and torso for the past few weeks. It is red, itchy, and covered in dry, scaly patches.  |   \n| Varicose Veins    | Long durations of standing or walking have caused severe discomfort in my legs. It's a burning ache that gets worse the longer I'm on my feet.|   \n  \n\n: Two data instances from symptom to disease dataset @mistral2024. The `text` column has the symptoms and `label` has corresponding disease name. {#tbl-sample-dataset}{tbl-colwidths=\"[25,75]\"}\n\n## Model Responses\n\nIn this section, I will use [`mistralai/Mistral-7B-Instruct-v0.3`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) to get diagnosis for each entry/symptom description in the dataset @tbl-sample-dataset. \n\nI am using `Mistral-7B-Instruct-v0.3` model because it is already fine-tuned to follow instructions. I also experimented a bit with Llama 3 7B though since it is not fine-tuned to follow instructions, the Llama responses comprised more of text generation than diagnosing a disease.\n\nFor this example, I included particular instructions I wanted `Mistral-7B-Instruct-v0.3` to follow for medical disgnostics in `SYSTEM_PROMPT`. In short, the instructions included to respond with a disease name that is most likely causing the symptoms as entered by a user. The complete `SYSTEM_PROMPT` is shown in @fig-system-prompt.\n\n::: {#fig-system-prompt}\n\n::: {#faf73854 .cell execution_count=2}\n``` {.python .cell-code}\nSYSTEM_PROMPT = \"\"\"\nYou are an expert medical professional named Medical AI. \n\nUsing the text provided by a user which is explaining symptoms of a disease, give your diagnosis. \n\nYou have to choose from one of the diseases below:\n\nPsoriasis                          \nVaricose Veins                     \npeptic ulcer disease               \ndrug reaction                      \ngastroesophageal reflux disease    \nallergy                            \nurinary tract infection            \nMalaria                            \nJaundice                           \nCervical spondylosis               \nMigraine                           \nHypertension                       \nBronchial Asthma                   \nAcne                               \nArthritis                          \nDimorphic Hemorrhoids              \nPneumonia                          \nCommon Cold                        \nFungal infection                   \nDengue                             \nImpetigo                           \nChicken pox                        \nTyphoid                            \ndiabetes  \n\nDo not add any other text except the disease name. \n\nFor example: \n\nUser input: The skin around my mouth, nose, and eyes is red and inflamed. It is often itchy and uncomfortable. There is a noticeable inflammation in my nails\nMedical AI: Psoriasis\n\nUser input:{{prompt}}\nMedical AI:\n\"\"\"\n```\n:::\n\n\n`SYSTEM_PROMPT` as used in the example to query `Mistral-7B-Instruct-v0.3` for getting diagnosis based on symptoms as entered by a user. The `{{prompt}}` will get replaced by symptoms from `text` field of dataset @mistral2024.\n:::\n\nTo use `Mistral-7B-Instruct-v0.3` to get the responses, I installed `mistral_inference`, and downloaded the model from Hugging Face. Complete code block for getting `Mistral-7B-Instruct-v0.3` response for each data instace in symptom to disease dataset is shown in @fig-code-block-mistral-inference.\n\n::: {#fig-code-block-mistral-inference}\n\n::: {#9af233a2 .cell execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\nfrom huggingface_hub import snapshot_download\nfrom pathlib import Path\nfrom mistral_inference.transformer import Transformer\nfrom mistral_inference.generate import generate\n\nfrom mistral_common.tokens.tokenizers.mistral import MistralTokenizer\nfrom mistral_common.protocol.instruct.messages import UserMessage\nfrom mistral_common.protocol.instruct.request import ChatCompletionRequest\n\nmistral_models_path = Path.home().joinpath('mistral_models', '7B-Instruct-v0.3')\nmistral_models_path.mkdir(parents=True, exist_ok=True)\n\nsnapshot_download(repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\", allow_patterns=[\"params.json\", \"consolidated.safetensors\", \"tokenizer.model.v3\"], local_dir=mistral_models_path)\n\ntokenizer = MistralTokenizer.from_file(f\"{mistral_models_path}/tokenizer.model.v3\")\nmodel = Transformer.from_folder(mistral_models_path, device = \"cuda\")\n\n# Data\n# Symptom2Disease.csd Dataset: !wget https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/data/Symptom2Disease.csv\ndataframe = pd.read_csv(\"Symptom2Disease.csv\")\nshuffled_dataframe = dataframe.sample(frac=1)\n\n\n# Iterate through prompts with progress bar\nfor index, row in tqdm(data.iterrows(), total=len(data)):\n    try:\n        # Get LLM response - modify this part according to your LLM API\n        prompt = SYSTEM_PROMPT.replace(\"{{prompt}}\", row[\"text\"])\n        completion_request = ChatCompletionRequest(messages=[UserMessage(content=prompt)])\n        tokens = tokenizer.encode_chat_completion(completion_request).tokens\n\n        out_tokens, _ = generate([tokens], model, max_tokens=64, temperature=0.0, eos_id=tokenizer.instruct_tokenizer.tokenizer.eos_id)\n        response_text = tokenizer.instruct_tokenizer.tokenizer.decode(out_tokens[0])\n        \n                \n        # Store original prompt and response\n        results.append({\n            'prompt': row['text'],\n            'response': response_text\n        })\n\n        print(response_text)\n        \n        # Optional: Add delay to respect rate limits\n        time.sleep(0.1)\n        \n    except Exception as e:\n        print(f\"Error processing prompt at index {index}: {str(e)}\")\n        # Store error in results\n        results.append({\n            'prompt': row['prompt'],\n            'response': f\"ERROR: {str(e)}\"\n        })\n\n# Create new dataframe with results\nresults_df = pd.DataFrame(results)\n\n# Save to CSV\nresults_df.to_csv('llm_responses.csv', index=False)\n```\n:::\n\n\nCode used for getting [`mistralai/Mistral-7B-Instruct-v0.3`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) responses for system to disease dataset @mistral2024.\n:::\n\n\n## Unit Evaluations \nThe first form of evaluations to consider are unit evaluations that ensure that our LLM is responding with expected response. For this example we are using [`mistralai/Mistral-7B-Instruct-v0.3`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3) to diagnose a disease based on given symptoms. In order to evaluate `Mistral-7B-Instruct-v0.3` responses for medical diagnosis using unit evaluation, we can test for following unit evaluations:\n\n1. Response is non null \n2. Response only contains strings \n3. Response has one of the disease names as outlined in `SYSTEM_PROMPT` in @fig-system-prompt.\n\nThough the first two are not domain specific evaluations, the third one can be deemed somewhat domain specific though still no domain knowledge is required for third evaluation per se. Hence for unit evaluations, we do not require to use another LLM as a judge just yet. An excerpt from code for unit evaluation is shown in @fig-unit-evals-code.\n\n::: {#fig-unit-evals-code}\n\n::: {#449f1e69 .cell execution_count=4}\n``` {.python .cell-code}\nclass TestSample:\n    def __init__(self, data: Dict):\n        self.data = data\n        self.test_results = {}\n    \n    def run_tests(self):\n        # Test 1: Non-Empty Response\n        self.test_results['non_empty_response'] = not pd.isna(self.data['response']) and self.data['response'] != ''\n        \n        # Test 2: String Type\n        self.test_results['all_string'] = isinstance(self.data['response'], str)\n        \n        # Test 3: Valid Disease Name\n        try:\n            if self.data['response'] in POSSIBLE_DISEASES or POSSIBLE_DISEASES in self.data['response']:                \n                self.test_results['valid_disease_name'] = True        \n        except:\n            self.test_results['valid_disease_name'] = False\n            \n        return self.test_results\n```\n:::\n\n\nAn excerpt that shows the unit evaluations being used to assert that response from LLM for diagnosis is non-empty, contains all strings, and contains one of the possible disease names. Further additions may also include for case insensitive tests. \n:::\n \nI ran the evaluation on all 1200 data samples, i.e. responses obtained from `Mistral-7B-Instruct-v0.3`, and saved results in a .csv file for later use purposes. I also generated a bar plot shown in @fig-log-file-unit-test-symptom2disease where we can see that all responses from `Mistral-7B-Instruct-v0.3` were non-empty, and contained strings only. Though around 600 samples failed to include a disease name in response. \n\n\n![Unit evaluations run on `Mistral-7B-Instruct-v0.3` model response passed non-empty response, and all string response, though almost half of responses failed to include a disease name.](images/unit_evaluations_plot.png){#fig-log-file-unit-test-symptom2disease} \n\nIn @fig-unit-eval-failed-samples, I have listed some responses that are failing the \"disease name in response\" test. A quick look tells that these responses did indeed have a disease name though failed the unit evaluations because the case didn't match between model response and the name of the disease in `POSSIBLE_DISEASES`. \n\n::: {#fig-unit-eval-failed-samples}\n\n```{markdown}\nResponse does not contain valid disease name:\n,response\n2,Gastroesophageal Reflux Disease (GERD)\n5,Peptic Ulcer Disease\n11,\"Psoriasis (for the first user input)\nVaricose Veins (for the second user input)\n(For the third user input, the symptoms provided are not specific to any of the listed diseases. It could be a sign of various conditions such as eczema, seborrheic\"\n12,Psoriatic Arthritis (This condition is a combination of psoriasis and arthritis.)\n13,Jaundice (possibly associated with Hepatitis or other liver diseases)\n15,Allergy\n17,Cervical Spondylosis\n18,Jaundice (possibly due to Hepatitis or Gallstones)\n20,Common Cold or Bronchial Asthma (depending on the persistence and severity of symptoms)\n24,Peptic Ulcer Disease\n25,Peptic Ulcer Disease\n28,Flu (Influenza)\n33,Peptic Ulcer Disease\n40,Peptic Ulcer Disease\n42,Peptic Ulcer Disease\n43,Common Cold or Flu (Influenza)\n46,Flu (Influenza)\n50,Peptic Ulcer Disease\n51,Peptic Ulcer Disease\n52,Common Cold or Allergy (further evaluation is needed to differentiate)\n\n```\n\n\nA few of the responses that failed the \"response contains a disease name\" test. As we can see that in each reponse there does seem to be a disease name though our unit test is case sensitive which is generating a lot of false positives.\n:::\n\nFor the next iteration of unit evaluations, I updated the test to be case insensitive and the new results are as shown in @fig-log-file-unit-test-case-insensitive-symptom2disease. There is a significant improvement in \"response contains a disease name\" test. Some of the responses that still failed the test are listed in @fig-unit-eval-failed-samples-v2.\n\n![Unit evaluations run on `Mistral-7B-Instruct-v0.3` model response passed non-empty response, and all string response, though almost half of responses failed to include a disease name.](images/unit_evaluations_plot_case_insensitive.png){#fig-log-file-unit-test-case-insensitive-symptom2disease} \n\n \n::: {#fig-unit-eval-failed-samples-v2}\n\n```{markdown}\nResponse does not contain valid disease name:\n,response\n2,Gastroesophageal Reflux Disease (GERD)\n11,\"Psoriasis (for the first user input)\nVaricose Veins (for the second user input)\n(For the third user input, the symptoms provided are not specific to any of the listed diseases. It could be a sign of various conditions such as eczema, seborrheic\"\n12,Psoriatic Arthritis (This condition is a combination of psoriasis and arthritis.)\n13,Jaundice (possibly associated with Hepatitis or other liver diseases)\n18,Jaundice (possibly due to Hepatitis or Gallstones)\n20,Common Cold or Bronchial Asthma (depending on the persistence and severity of symptoms)\n28,Flu (Influenza)\n43,Common Cold or Flu (Influenza)\n46,Flu (Influenza)\n52,Common Cold or Allergy (further evaluation is needed to differentiate)\n61,\"Lymphadenopathy (could be caused by various conditions, further investigation is needed)\"\n74,Varicose Veins (again)\n\n```\n\n\nA few of the responses that failed the \"response contains a disease name\" test. As we can see that in each reponse there does seem to be a disease name though some samples do not seem to have same disease name as in `POSSIBLE_DISEASE` list such as `Flu`. \n:::\n\nIn a furture iteration, we can perhaps add synonyms for common disease such as `Flu` is same as `Cold`, or `Common Cold`. Next, I will cover LLM as a Judge for evaluating `Mistral-7B-Instruct-v0.3` responses.\n\n\n## LLM as a Judge \n\nFor using an LLM as a judge, we can query another, more powerful LLM and \"make it a judge\" to assess the response quality. Inspect support most of the popular models such as OpenAI, Anthropic, Google, Mistral. For a complete list of supported models please read [here](https://inspect.ai-safety-institute.org.uk/models.html). Please note, we will need API key for the model we want to use as our judge. \n\nThe critique prompt should include background and instructions for the judge LLM to use as a guide for its critique of `Mistral-7B-Instruct-v0.3` responses. @fig-critique-text is an excerpt from critique prompt that I used in this example though please note the critique prompt is generated using an LLM and may not be representative of best practices in medical diagnostics. \n\n\n\n::: {#fig-critique-text}\n\n```{markdown}\n## Background\n\nA medical diagnosis system should evaluate symptoms and provide accurate disease predictions. The system should consider:\n\n1. Primary symptoms (major indicators of the disease)\n2. Secondary symptoms (supporting symptoms)\n3. Location and distribution of symptoms\n4. Symptom severity, duration, and progression\n5. Common comorbidities and complications\n6. Risk factors and patient demographics\n7. Seasonal patterns (when applicable)\n8. Symptom triggers and alleviating factors\n\n## Instructions\n\nYou are an EXPERT medical professioanl evaluating symptom-to-disease predictions. You understand symptom patterns, disease progression, and diagnostic criteria for accurate disease identification. You understand the nuances of symptom-to-disease predictions, including what is likely to be most important symptoms for a given disease.\n\nYou are given the following two inputs: (1) SYMPTOMS, (2) PREDICTION. Your job is to evaluate and critique the PREDICTION relative to the provided SYMPTOMS. \n\nThe critiques must be provided in the same json format as provided in the examples below:\n\nInputs:\n1. Patient symptom description (SYMPTOMS)\n2. Predicted disease (PREDICTION)\n\nThe critique system should evaluate:\n- Symptom pattern matching\n- Presence of disease-specific indicators\n- Appropriate symptom timing and progression\n- Correct interpretation of constitutional symptoms\n- Recognition of complicating factors\n- Proper differentiation from similar conditions\n- Consideration of risk factors and demographics\n- Identification of red flags requiring immediate attention\n\nFor the below SYMPTOMS and PREDICTION provide a critique as JSON in the format {\"critique\": \"...\", \"outcome\": \"good\"|\"bad\"} as shown above. Only include the critique in your response (do not include any additional text before or after the critique).\n\nOutcomes should be marked \"good\" when predictions accurately match symptom patterns and \"bad\" when key diagnostic features are missed or misinterpreted.\n\n\n---\n\nSYMPTOMS: \"I've been having severe stomach pain, especially when hungry. The pain improves after eating but comes back after a few hours. I sometimes feel nauseous and have noticed dark stools.\"\n\nPREDICTION: Peptic Ulcer Disease\n\n{\"critique\": \"Excellent prediction. Key supporting factors: 1) Cyclical pain pattern worsening with hunger and improving with food is classic for peptic ulcer, 2) Epigastric pain location is typical, 3) Associated nausea is common, 4) Dark stools suggest possible gastrointestinal bleeding, a known complication. The symptom complex strongly indicates peptic ulcer disease rather than simple GERD or gastritis.\", \"outcome\": \"good\"}\n\n---\n```\n\nSome exceprt from critique prompt that I used for LLM as a judge. Disclaimer: I have used an LLM to generate example critiques which may not be representative of real medical diagnostics. This critique is to just show case how one LLM can be used as a judge to evaluate another LLMs responses.\n\n:::\n\nI used OpenAI `gpt4` model as a judge to critique `Mistral-7B-Instruct-v0.3` responses. @fig-log-file-llm-judge-symptom2disease outlines critique obtained on one sample where `gpt4` and `Mistral-7B-Instruct-v0.3` responses are not in agreement. \n\nThe critique from an LLM can also be used in tandem with subject experts, medical professionals in this case, to curate data for fine-tuning. Though this should be used with caution as it can help propagate human or LLM bias in fine-tuning dataset which in turn would result in skewed responses from a fine-tuned model.\n\n![A sample showing critique for a medical diagnosis that judge LLM thought was a poor diagnosis. It includes explanation for its critique. ](images/critique_sample_log_file.png){#fig-log-file-llm-judge-symptom2disease}\n\n# Conclusion\n\nIn this blog, I covered my notes on LLM evaluations from the course Mastering large language model (LLM) For Developers & Data Scientists [@mastering_llms_2024]. LLM evaluations provide a framework for evaluating the performance of an LLM. It might help to think of LLM evaluations as an iterative process which helps improve our understanding of LLM responses/data we are evaluating. A better understanding of LLM responses helps write better LLM evaluations, which in turn helps to understand LLM responses by gaining insights from where LLM evaluations failed and passed. \n\nAs LLMs evolve, it is important to think of more nuanced evaluation frameworks that consider reasoning, truthfulness, and real-world applicability. Also, an appreciation  that no single metric can fully capture an LLM's capabilities – instead, a holistic approach combining quantitative benchmarks, qualitative assessments, and domain-specific testing will be crucial. \n\n\n# References {-}\n\n::: {#refs}\n:::\n\n",
    "supporting": [
      "blog_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}